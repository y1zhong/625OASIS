```{r}
library(ggplot2)
library(glmnet)
library(dplyr)
library(knitr)
library(caret)
library(tictoc)
```

```{r}
#--------------------------------------------------------------------
#   Select slice 88, create 3d array (176, 208, 198)
#--------------------------------------------------------------------
setwd("~/Documents/GitHub/625OASIS")
load("data/img_list.rds")
a = img_list[[1]]                           #read in 3D array 176x208x176
S.new <- array (NA, dim = c(176, 208, length(img_list)))

for (i in 1 : length(img_list)){            #select slice 88 and create array 176x208x198
  a = img_list[[i]]
  test = a[, , 88]
  S.new[, , i] = test
}

#--------------------------------------------------------------------
#   Remove rows and columns with all 0
#--------------------------------------------------------------------
S.new =  S.new[apply(S.new != 0, 1, any),   #137x169x198
               apply(S.new != 0, 2, any), ]
load("~/Documents/GitHub/625OASIS/data/oasis_csdt.Rdata")
labels = oasis_csdt$CDR
```

```{r}
#--------------------------------------------------------------------
#   Partition Train and Test Data 8:2
#--------------------------------------------------------------------
n = dim(S.new)[3]   #from n = 198
num_lst = 1:n
set.seed(123)
random_sample <- createDataPartition(num_lst, p = 0.8, list = FALSE)

train.X = S.new[, , random_sample]
test.X  = S.new[, , - random_sample]

train.demo = oasis_csdt[random_sample, ]
test.demo  = oasis_csdt[ - random_sample, ]

train.Y = train.demo$CDR
test.Y  = test.demo$CDR
```

```{r, out.width="100px"}
#--------------------------------------------------------------------
#   Visualize Image
#--------------------------------------------------------------------
for (i in c(1, 3)){
  image(train.X[, , i], col = gray(c(0:255)/255))
}
```
```{r}
table(train.Y)
table(test.Y)
```
```{r}
#return 2D array
prep_binary_classification = function(images, labels, posLabel = 1, negLabel = 0) {
  dims = dim(images)
  X.pos = t( matrix(images[, , labels %in% posLabel], nrow = dims[1] * dims[2]) ) ## n1 x (137x169)
  X.neg = t( matrix(images[, , labels %in% negLabel], nrow = dims[1] * dims[2]) ) ## n1 x (137x169)
  return( list(X=rbind(X.pos, X.neg),
              y = c(rep(1,nrow(X.pos)), rep(0,nrow(X.neg)))))
}
```

```{r}
trn = prep_binary_classification(train.X, train.Y, 1, 0)
tst = prep_binary_classification(test.X, test.Y, 1, 0)

## Check the dimensions of the data
print( dim(trn$X) )
print( length(trn$y) )
print( dim(tst$X) )
print( length(tst$y) )

#prepare whole data for cross-validation
combined = prep_binary_classification (S.new, oasis_csdt$CDR, 1, 0)
print( dim(combined$X) ) 
print( length(combined$y) )
```
```{r}
#--------------------------------------------------------------------
#   Cross Validation Ridge
#--------------------------------------------------------------------
tic()
cv.fit.ridge = cv.glmnet(combined$X, combined$y, family = "binomial", 
                   alpha = 0, 
                   standardize = FALSE, keep = TRUE)
param.ridge = cv.fit.ridge$lambda.min             #best lambda parameter
coef.ridge  = coef(cv.fit.ridge, s = "lambda.min")
toc()

```

```{r}
#--------------------------------------------------------------------
#   Ridge Regression
#--------------------------------------------------------------------
tic()
ridge.fit <- glmnet(trn$X, trn$y, family = "binomial", alpha = 0, lambda = param.ridge)
ridge.pred.tst <- predict(ridge.fit, tst$X, type = "response")
kable(data.frame(pred = as.integer(ridge.pred.tst > 0.5), true = tst$y) %>% count(true, pred) %>%
        mutate(freq = n / sum(n)))      #test data
toc()

confusion.ridge <- confusion.glmnet(ridge.fit, newx = tst$X, newy = tst$y)
confusion.ridge
```


```{r}
#--------------------------------------------------------------------
#   Cross Validation Lasso
#--------------------------------------------------------------------
tic()
cv.fit.lasso = cv.glmnet(combined$X, combined$y, family = "binomial", 
                   alpha = 1, 
                   standardize = FALSE, keep = TRUE)
param.lasso = cv.fit.lasso$lambda.min   #best parameter
coef.lasso = coef(cv.fit.lasso, s = "lambda.min")
toc()
```

```{r}
#--------------------------------------------------------------------
#   Lasso Regression
#--------------------------------------------------------------------
tic()
lasso.fit <- glmnet(trn$X, trn$y, family = "binomial", alpha = 1, lambda = param.lasso)
lasso.pred.tst <- predict(lasso.fit, tst$X, type = "response")
kable(data.frame(pred = as.integer(lasso.pred.tst > 0.5), true=tst$y) %>% count(true, pred) %>% 
        mutate(freq = n / sum(n)))
toc()

confusion.lasso <- confusion.glmnet(lasso.fit, newx = tst$X, newy = tst$y)
confusion.lasso
```


```{r}
#--------------------------------------------------------------------
#   Cross Validation Plots
#--------------------------------------------------------------------
idmin.ridge = match(cv.fit.ridge$lambda.min, cv.fit.ridge$lambda)
idmin.lasso = match(cv.fit.lasso$lambda.min, cv.fit.lasso$lambda)

roc.ridge = roc.glmnet(cv.fit.ridge$fit.preval, newy = combined$y)[[idmin.ridge]]
roc.lasso = roc.glmnet(cv.fit.lasso$fit.preval, newy = combined$y)[[idmin.lasso]]

plot(roc.ridge)
plot(roc.lasso)

```

```{r}
#save output
save(cv.fit.ridge, ridge.fit, confusion.ridge, roc.ridge, file = "Ridge_ImgOnly.RData")
save(cv.fit.lasso, lasso.fit, confusion.lasso, roc.lasso, file = "Lasso_ImgOnly.RData")
```

==============================================================================
With Demographics
==============================================================================

```{r}
#remove CDR from data
train.demo2 = subset(train.demo, select = -c(CDR))
test.demo2 = subset(test.demo, select = -c(CDR))
data2 = subset(oasis_csdt, select = -c(CDR))

train.demo2$M.F = ifelse(train.demo2$M.F == "F", 1, 0)
test.demo2$M.F  = ifelse(test.demo2$M.F == "F", 1, 0)
data2$M.F       = ifelse(data2$M.F == "F", 1, 0)

demo.trn.X = data.matrix( cbind(train.demo2, trn$X) )
demo.tst.X = data.matrix( cbind(test.demo2, tst$X) )
demo.combined.X = data.matrix( cbind(data2, combined$X) )

print (dim(demo.trn.X))
print (dim(demo.tst.X))
print (dim(demo.combined.X))
```

```{r}
#--------------------------------------------------------------------
#   Cross Validation Ridge
#--------------------------------------------------------------------
tic()
cv.fit.demo = cv.glmnet(demo.combined.X, combined$y, family = "binomial", 
                   alpha = 0, 
                   standardize = FALSE, keep = TRUE)
param.demo = cv.fit.demo$lambda.min   #best parameter
coef.demo = coef(cv.fit.demo, s = "lambda.min")
toc()
```

```{r}
#--------------------------------------------------------------------
#   Ridge Regression
#--------------------------------------------------------------------
tic()
demo.fit <- glmnet(demo.trn.X, trn$y, family = "binomial", alpha = 0,
                   lambda = param.demo)
demo.pred.tst <- predict(demo.fit, data.matrix(demo.tst.X), type = "response")
kable(data.frame(pred = as.integer(demo.pred.tst > 0.5), true = tst$y) %>% count(true,pred) %>%
        mutate(freq = n / sum(n)))      #test data
toc()

confusion.demo <- confusion.glmnet(demo.fit, newx = demo.tst.X, newy = tst$y)
confusion.demo

idmin.demo = match(cv.fit.demo$lambda.min, cv.fit.demo$lambda)
roc.demo = roc.glmnet(cv.fit.demo$fit.preval, newy = combined$y)[[idmin.demo]]
plot(roc.demo)

```
```{r}
#save output
save(cv.fit.demo, demo.fit, confusion.demo, roc.demo, file = "Ridge_ImgCS.RData")
```

==============================================================================
Selecting More than one Slice
==============================================================================
```{r}
S.five <- array (NA, dim = c(352, 416, length(img_list)))

for (i in 1 : length(img_list)){            #select slice 88 and create array 176x208x198
  a = img_list[[i]]
  test2 = a[, , 95]
  test3 = a[, , 90]
  test4 = a[, , 85]
  test5 = a[, , 80]
  S.five[, , i] = rbind(cbind(test2,test3), cbind(test4,test5))
}

#--------------------------------------------------------------------
#   Remove rows and columns with all 0
#--------------------------------------------------------------------
S.five =  S.five[apply(S.five != 0, 1, any),   #274x340x198
               apply(S.five != 0, 2, any), ]

```


```{r}
#--------------------------------------------------------------------
#   Separate Train and Test Data 8:2 (Five Slices)
#--------------------------------------------------------------------
train.X.2 = S.five[, , random_sample]
test.X.2  = S.five[, , - random_sample]

trn.2 = prep_binary_classification(train.X.2, train.Y, 1, 0)
tst.2 = prep_binary_classification(test.X.2, test.Y, 1, 0)

## Check the dimensions of the data
print( dim(trn.2$X) )
print( length(trn.2$y) )
print( dim(tst.2$X) )
print( length(tst.2$y) )

combined.2 = prep_binary_classification (S.five, oasis_csdt$CDR, 1, 0)
print( dim(combined.2$X) ) 
print( length(combined.2$y) )
```
```{r}
#--------------------------------------------------------------------
#   Cross Validation Ridge (Five Slices)
#--------------------------------------------------------------------
tic()
cv.fit.five = cv.glmnet(combined.2$X, combined.2$y, family = "binomial", 
                   alpha = 0, 
                   standardize = FALSE, keep = TRUE)
param.five = cv.fit.five$lambda.min             #best lambda parameter
coef.five  = coef(cv.fit.five, s = "lambda.min")
toc()

```

```{r}
#--------------------------------------------------------------------
#   Ridge Regression (Five Slices)
#--------------------------------------------------------------------
tic()
five.fit <- glmnet(trn.2$X, trn.2$y, family = "binomial", alpha = 0, lambda = param.five)
five.pred.tst <- predict(five.fit, tst.2$X, type = "response")
kable(data.frame(pred = as.integer(five.pred.tst > 0.5), true = tst.2$y) %>% count(true, pred) %>%
        mutate(freq = n / sum(n)))      #test data
toc()

confusion.five <- confusion.glmnet(five.fit, newx = tst.2$X, newy = tst.2$y)
confusion.five
```








