```{r}
library(ggplot2)
library(glmnet)
library(caret)
library(dplyr)
library(knitr)
```

```{r}
#--------------------------------------------------------------------
#   Select slice 88, create 3d array (176, 208, 198)
#--------------------------------------------------------------------
setwd("/Users/ginj/Documents/GitHub/625OASIS")
load("data/img_list.rds")
a = img_list[[1]]
# dim(a) = c(176, 176 * 208)
# a = t(a)
# img = c()
S.new <- array (NA, dim = c(176,208,length(img_list)))

for (i in 1:length(img_list)){
  a = img_list[[i]]
  test = a[,,88]
  S.new[,,i] = test
}

#--------------------------------------------------------------------
#   Remove rows and columns with all 0
#--------------------------------------------------------------------
S.new =  S.new[apply(S.new != 0, 1, any), apply(S.new != 0, 2, any) ,]
setwd("/Users/ginj/Documents/GitHub/625OASIS")
data = read.csv("data/oasis_cross-sectional_filter.csv")
#data = oasis_cross_sectional_filter
labels = data$CDR


```

```{r}
#--------------------------------------------------------------------
#   Separate Train and Test Data 8:2
#--------------------------------------------------------------------
n = dim(S.new)[3]   #from n = 198
num_lst = 1:n
set.seed(123)
random_sample <- createDataPartition(num_lst, p = 0.8, list = FALSE)

train.X = S.new[,,random_sample]
test.X = S.new[,,-random_sample]

train.demo = data[random_sample,]
test.demo = data[-random_sample,]

train.Y = train.demo$CDR
test.Y = test.demo$CDR
```

```{r}
#--------------------------------------------------------------------
#   Visualize Image
#--------------------------------------------------------------------
for (i in c(1, 3)){
  image(train.X[,,i])
}
```
```{r}
table(train.Y)
table(test.Y)
```
```{r}
prep_binary_classification = function(images, labels, posLabel = 1, negLabel = 0) {
  dims = dim(images)
  X.pos = t(matrix(images[,,labels %in% posLabel],nrow=dims[1]*dims[2])) ## n1 * (137*169)
  X.neg = t(matrix(images[,,labels %in% negLabel],nrow=dims[1]*dims[2])) ## n1 * (137*169)
  return(list(X=rbind(X.pos, X.neg),y = c(rep(1,nrow(X.pos)),rep(0,nrow(X.neg)))))
}
```

```{r}
trn = prep_binary_classification(train.X, train.Y, 1, 0)
tst = prep_binary_classification(test.X, test.Y, 1, 0)
## Check the dimensions of the data
print(dim(trn$X))
print(length(trn$y))
print(dim(tst$X))
print(length(tst$y))

combined = prep_binary_classification (S.new, data$CDR, 1, 0)
print (dim(combined$X))
print (length(combined$y))
```
```{r}
#--------------------------------------------------------------------
#   Cross Validation Ridge
#--------------------------------------------------------------------
cv.fit.ridge = cv.glmnet(combined$X, combined$y, family = "binomial", 
                   alpha = 0, 
                   standardize = FALSE, keep = TRUE)
#cbind(cv.fit.ridge$lambda, cv.fit.ridge$cvm)
param.ridge = cv.fit.ridge$lambda.min   #best parameter
coef.ridge = coef(cv.fit.ridge, s = "lambda.min")

```

```{r}
#--------------------------------------------------------------------
#   Ridge Regression
#--------------------------------------------------------------------
ridge.fit <- glmnet(trn$X, trn$y, family="binomial", alpha = 0, lambda = param.ridge)
# ridge.pred.trn <- predict(ridge.fit, trn$X, type="response")
ridge.pred.tst <- predict(ridge.fit, tst$X, type="response")
# print(data.frame(pred=as.integer(ridge.pred.trn > 0.5), true=trn$y) %>% count(true,pred) %>%    mutate(freq=n/sum(n)))      #train data
kable(data.frame(pred=as.integer(ridge.pred.tst > 0.5), true=tst$y) %>% count(true,pred) %>%    mutate(freq=n/sum(n)))      #test data

confusion.ridge <- confusion.glmnet(ridge.fit, newx = tst$X, newy = tst$y)

```


```{r}
#--------------------------------------------------------------------
#   Cross Validation Lasso
#--------------------------------------------------------------------
cv.fit.lasso = cv.glmnet(combined$X, combined$y, family = "binomial", 
                   alpha = 1, 
                   standardize = FALSE, keep = TRUE)
#cbind(cv.fit.lasso$lambda, cv.fit.lasso$cvm)
param.lasso = cv.fit.lasso$lambda.min   #best parameter
coef.lasso = coef(cv.fit.lasso, s = "lambda.min")

```

```{r}
#--------------------------------------------------------------------
#   Lasso Regression
#--------------------------------------------------------------------
lasso.fit <- glmnet(trn$X, trn$y, family="binomial", alpha = 1, lambda = param.lasso)
#print (lasso.fit)
# lasso.pred.trn <- predict(lasso.fit, trn$X, type="response")
lasso.pred.tst <- predict(lasso.fit, tst$X, type="response")
# print(data.frame(pred=as.integer(lasso.pred.trn > 0.5), true=trn$y) %>% count(true,pred) %>% mutate(freq=n/sum(n)))
kable(data.frame(pred=as.integer(lasso.pred.tst > 0.5), true=tst$y) %>% count(true,pred) %>% mutate(freq=n/sum(n)))

confusion.lasso <- confusion.glmnet(lasso.fit, newx = tst$X, newy = tst$y)
```


```{r}
#--------------------------------------------------------------------
#   Cross Validation Plots
#--------------------------------------------------------------------
idmin.ridge = match(cv.fit.ridge$lambda.min, cv.fit.ridge$lambda)
idmin.lasso = match(cv.fit.lasso$lambda.min, cv.fit.lasso$lambda)

roc.ridge = roc.glmnet(cv.fit.ridge$fit.preval, newy = combined$y)[[idmin.ridge]]
roc.lasso = roc.glmnet(cv.fit.lasso$fit.preval, newy = combined$y)[[idmin.lasso]]

plot(roc.ridge)
plot(roc.lasso)

confusion.glmnet(ridge.fit, newx = tst$X, newy = tst$y, s = 0.1)
confusion.glmnet(lasso.fit, newx = tst$X, newy = tst$y, s = 0.1)

```

```{r}
#save output
save(cv.fit.ridge, ridge.fit, confusion.ridge, roc.ridge, file = "Ridge_ImgOnly.RData")
save(cv.fit.lasso, lasso.fit, confusion.lasso, roc.lasso, file = "Lasso_ImgOnly.RData")
```

==============================================================================
With Demographics
==============================================================================

```{r}
#remove CDR from data
train.demo2 = subset(train.demo, select = -c(ID, Hand, CDR,SES,Delay))
test.demo2 = subset(test.demo, select = -c(ID, Hand, CDR,SES,Delay))
data2 = subset(data, select = -c(ID, Hand, CDR,SES,Delay))

train.demo2$M.F = ifelse(train.demo2$M.F == "F", 1, 0)
test.demo2$M.F = ifelse(test.demo2$M.F == "F", 1, 0)
data2$M.F = ifelse(data2$M.F == "F", 1, 0)

demo.trn.X = data.matrix( cbind(train.demo2, trn$X) )
demo.tst.X = data.matrix( cbind(test.demo2, tst$X) )
demo.combined.X = data.matrix( cbind(data2, combined$X) )

print (dim(demo.trn.X))
print (dim(demo.tst.X))
print (dim(demo.combined.X))
```

```{r}
#--------------------------------------------------------------------
#   Cross Validation Ridge
#--------------------------------------------------------------------
cv.fit.demo = cv.glmnet(demo.combined.X, combined$y, family = "binomial", 
                   alpha = 0, 
                   standardize = FALSE, keep = TRUE)
#cbind(cv.fit.demo$lambda, cv.fit.demo$cvm)
param.demo = cv.fit.demo$lambda.min   #best parameter
coef.demo = coef(cv.fit.demo, s = "lambda.min")
```

```{r}
#--------------------------------------------------------------------
#   Ridge Regression
#--------------------------------------------------------------------
demo.fit <- glmnet(data.matrix(demo.trn.X), trn$y, family="binomial", alpha = 0,
                   lambda = param.demo)
# ridge.pred.trn <- predict(demo.fit, trn$X, type="response")
demo.pred.tst <- predict(demo.fit, data.matrix(demo.tst.X), type="response")
# print(data.frame(pred=as.integer(ridge.pred.trn > 0.5), true=trn$y) %>% count(true,pred) %>%    mutate(freq=n/sum(n)))      #train data
kable(data.frame(pred=as.integer(demo.pred.tst > 0.5), true=tst$y) %>% count(true,pred) %>%    mutate(freq=n/sum(n)))      #test data

confusion.demo <- confusion.glmnet(demo.fit, newx = data.matrix(demo.tst.X), newy = tst$y)
confusion.demo

```
















